{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:21.283660Z",
     "start_time": "2019-02-12T16:04:20.720011Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ssl\n",
    "import math\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Structure for Categorical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:21.287728Z",
     "start_time": "2019-02-12T16:04:21.284848Z"
    }
   },
   "outputs": [],
   "source": [
    "class categorical_feature:\n",
    "    def __init__(self,name):\n",
    "        self.attribute_name = name\n",
    "        self.children = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Structure of Continuous Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.379898Z",
     "start_time": "2019-02-12T16:04:21.289040Z"
    }
   },
   "outputs": [],
   "source": [
    "class continuous_feature:\n",
    "    def __init__(self,name):\n",
    "        self.attribute_name = name\n",
    "        self.mean = {}\n",
    "        self.sd = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.409934Z",
     "start_time": "2019-02-12T16:04:22.386687Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_preprocessing():\n",
    "    \n",
    "    global feature_list, feature_type\n",
    "    \n",
    "    feature_list = [\"ID\",\"Age\",\"Experience\",\"Income\",\"Zipcode\",\"Family_Size\",\"Spending_Per_Month\",\"Education_Level\",\"Mortgage_Value\",\"Label\",\"Securities_Account\",\"Security_Deposits\",\"Internet_Banking\",\"Credit_Card\"]\n",
    "    \n",
    "    df = pd.read_csv(\"loan_dataset.csv\", names = feature_list,skiprows = 1)\n",
    "    \n",
    "    df[\"temp\"] = df.Label\n",
    "    df = df.drop([\"Label\"], axis=1)\n",
    "    df[\"label\"] = df.temp\n",
    "    df = df.drop([\"temp\"], axis=1)\n",
    "    \n",
    "    df = df.drop([\"ID\",\"Zipcode\"], axis=1)\n",
    "    \n",
    "    feature_list = [\"Age\",\"Experience\",\"Income\",\"Family_Size\",\"Spending_Per_Month\",\"Education_Level\",\"Mortgage_Value\",\"Securities_Account\",\"Security_Deposits\",\"Internet_Banking\",\"Credit_Card\",\"label\"]\n",
    "    \n",
    "    feature_type = classify_features(df)\n",
    "    \n",
    "    pos_df = df.loc[df['label'] == 0]\n",
    "    neg_df = df.loc[df['label'] == 1]\n",
    "    \n",
    "    #spliting positive and negative dataset into randomly 80-20 % split\n",
    "    pos_train_df, pos_test_df = train_test_split_data(pos_df, 0.2)\n",
    "    neg_train_df, neg_test_df = train_test_split_data(neg_df, 0.2)\n",
    "    \n",
    "    #merging positive and negative data split so that training and validation dataset contains equal number of positive and negative value of feature label \n",
    "    train_df = pd.concat([pos_train_df, neg_train_df])\n",
    "    test_df = pd.concat([pos_test_df, neg_test_df])\n",
    "    \n",
    "    return train_df, test_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Train and Test Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.422009Z",
     "start_time": "2019-02-12T16:04:22.412083Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split_data(df, size):\n",
    "    \n",
    "    if isinstance(size, float):\n",
    "        size = round(size * len(df))\n",
    "    \n",
    "    #getting indexes of dataset in a list\n",
    "    indices = df.index.tolist()\n",
    "    \n",
    "    #randomly choosing \"size\" number of indices for validation set\n",
    "    indices = random.sample(population=indices, k=size)\n",
    "\n",
    "    #Creating validation set\n",
    "    validation_df = df.loc[indices]\n",
    "    \n",
    "    #Creating trianing set\n",
    "    train_df = df.drop(indices)\n",
    "    \n",
    "    return train_df, validation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Features into Continuous and Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.437609Z",
     "start_time": "2019-02-12T16:04:22.423789Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_features(df):\n",
    "    \n",
    "    feature_type = []\n",
    "    \n",
    "    unique_values_treshold = 5\n",
    "    \n",
    "    for feature in df.columns:\n",
    "            \n",
    "        #Find unique values of a features\n",
    "        unique_values = df[feature].unique()\n",
    "        value = unique_values[0]\n",
    "\n",
    "        #if value is string then it has to be categorical or if no of unique values is less than threshold\n",
    "        if (isinstance(value, str)) or (len(unique_values) <= unique_values_treshold):\n",
    "            feature_type.append(\"categorical\")\n",
    "        else:\n",
    "            feature_type.append(\"continuous\")\n",
    "    \n",
    "    return feature_type\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.449510Z",
     "start_time": "2019-02-12T16:04:22.439266Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_classifier(train_df):\n",
    "    \n",
    "    global feature_summary\n",
    "    \n",
    "    feature_summary = summarize_dataset(train_df.values)\n",
    "    \n",
    "    return feature_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.466548Z",
     "start_time": "2019-02-12T16:04:22.451262Z"
    }
   },
   "outputs": [],
   "source": [
    "def summarize_dataset(data):\n",
    "    \n",
    "    feature_summary = []\n",
    "    no_of_columns = data.shape[1]\n",
    "    \n",
    "    #Looping on all features and storing all unique values as split points except on left feature\n",
    "    for column_index in range(no_of_columns-1):          \n",
    "        \n",
    "        if feature_type[column_index] == \"categorical\":\n",
    "                \n",
    "            categorical_obj = categorical_feature(feature_list[column_index])\n",
    "\n",
    "            values = data[:, column_index]\n",
    "            unique_values = np.unique(values)\n",
    "\n",
    "            for value in unique_values:\n",
    "\n",
    "                categorical_obj.children[value] = calculate_probability_categorical(data,column_index,value)\n",
    "            \n",
    "            feature_summary.append(categorical_obj)      \n",
    "                    \n",
    "        elif feature_type[column_index] == \"continuous\":\n",
    "        \n",
    "            continuous_obj = continuous_feature(feature_list[column_index])\n",
    "            \n",
    "            values = data[:, -1]\n",
    "            unique_values = np.unique(values)\n",
    "            \n",
    "\n",
    "            for value in unique_values:\n",
    "\n",
    "                continuous_obj.mean[(value)] = (calculate_mean(data,column_index,value)) \n",
    "                continuous_obj.sd[(value)] = (calculate_sd(data,column_index,value))\n",
    "            \n",
    "            feature_summary.append(continuous_obj)\n",
    "    \n",
    "    return feature_summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.482022Z",
     "start_time": "2019-02-12T16:04:22.468223Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_probability_categorical(data,column_index,feature_value):\n",
    "\n",
    "    values_list = {}\n",
    "\n",
    "    label_column = data[:, -1]\n",
    "    \n",
    "    label_unique_values , label_counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "\n",
    "    for index in range(len(label_unique_values)):\n",
    "        \n",
    "        temp = data[label_column == label_unique_values[index]]\n",
    "        \n",
    "        column_values = temp[:, column_index]\n",
    "        \n",
    "        unique_values , counts = np.unique(column_values, return_counts=True)\n",
    "        \n",
    "        unique_values = list(unique_values)\n",
    "        \n",
    "        feature_index = unique_values.index(feature_value)\n",
    "    \n",
    "        count_feature_value = counts[feature_index]\n",
    "    \n",
    "        values_list[(label_unique_values[index])] = count_feature_value/label_counts[index]\n",
    "    \n",
    "    \n",
    "    return values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.496865Z",
     "start_time": "2019-02-12T16:04:22.483340Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_probability_continuous(column_index,feature_value,label_value):\n",
    "\n",
    "    mean = feature_summary[column_index].mean[(label_value)]\n",
    "    sd = feature_summary[column_index].sd[(label_value)]\n",
    "    \n",
    "    exponent = math.exp(-(math.pow(feature_value-mean,2)/(2*math.pow(sd,2))))\n",
    "\n",
    "    return (1/(math.sqrt(2*math.pi)*sd))*exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean of continuous feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.508943Z",
     "start_time": "2019-02-12T16:04:22.498141Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mean(data,column_index,label_value):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    data = data[label_column == label_value]\n",
    "\n",
    "    column_values = data[:, column_index]\n",
    "    \n",
    "    return np.mean(column_values,dtype=np.float64)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:08:13.191588Z",
     "start_time": "2019-02-12T16:08:13.185419Z"
    }
   },
   "source": [
    "### Calculate Standard Deviation of continuous feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.561167Z",
     "start_time": "2019-02-12T16:04:22.510680Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_sd(data,column_index,label_value):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    data = data[label_column == label_value]\n",
    "\n",
    "    column_values = data[:, column_index]\n",
    "    \n",
    "    return np.std(column_values,dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying test dataset based on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.579343Z",
     "start_time": "2019-02-12T16:04:22.562857Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_data(test_row,label_column):\n",
    "    \n",
    "    global feature_summary\n",
    "    \n",
    "    probablility_list = {}\n",
    "    \n",
    "    label_unique_values , counts = np.unique(label_column, return_counts=True)\n",
    "    label_index = 0\n",
    "    \n",
    "    for label_unique in label_unique_values:\n",
    "    \n",
    "        probability = 1\n",
    "    \n",
    "        for index in range(len(feature_list)-1):\n",
    "            \n",
    "            if feature_type[index] == \"categorical\":\n",
    "                \n",
    "                probability *= feature_summary[index].children[(test_row[feature_list[index]])][(label_unique)]\n",
    "                \n",
    "            elif feature_type[index] == \"continuous\":\n",
    "\n",
    "                probability *= calculate_probability_continuous(index,test_row[feature_list[index]],label_unique)\n",
    "    \n",
    "        probability *= counts[label_index]/counts.sum()\n",
    "    \n",
    "        probablility_list[(label_unique)] = probability\n",
    "        \n",
    "        label_index += 1\n",
    "        \n",
    "    sum_values = 0\n",
    "    label_answer = \"\"\n",
    "    max_value = float('-inf')\n",
    "    \n",
    "    for item, values in probablility_list.items():\n",
    "        \n",
    "        sum_values += values\n",
    "        \n",
    "    for item, values in probablility_list.items():\n",
    "        if max_value < (float)(values/sum_values):\n",
    "            \n",
    "            max_value = (float)(values/sum_values)\n",
    "            label_answer = (int(item))\n",
    "            \n",
    "    return label_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.593063Z",
     "start_time": "2019-02-12T16:04:22.580782Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(df):\n",
    "    \n",
    "    #to count true positive\n",
    "    count_TP = 0\n",
    "    \n",
    "    #to count false positive\n",
    "    count_FP = 0\n",
    "    \n",
    "    #to count false negative\n",
    "    count_FN = 0\n",
    "    \n",
    "    #to count true negative\n",
    "    count_TN = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"result\"] == row[\"label\"] and row[\"label\"] == 1:\n",
    "            count_TP += 1\n",
    "        elif row[\"result\"] == row[\"label\"] and row[\"label\"] == 0:\n",
    "            count_TN += 1    \n",
    "        elif row[\"result\"] == 1 and row[\"label\"] == 0:\n",
    "            count_FP += 1\n",
    "        elif row[\"result\"] == 0 and row[\"label\"] == 1:    \n",
    "            count_FN += 1\n",
    "            \n",
    "    print(\"True Positive: \", count_TP)\n",
    "    print(\"True Negative: \", count_TN)\n",
    "    print(\"False Positive: \", count_FP)\n",
    "    print(\"False Negative: \", count_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.607017Z",
     "start_time": "2019-02-12T16:04:22.594204Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_f1_score(df):\n",
    "    \n",
    "    precision = calculate_precision(df)\n",
    "    recall = calculate_recall(df)\n",
    "    \n",
    "    #If recall and precision is both 0 then f1 score is undefined\n",
    "    if precision == 0 or recall == 0:\n",
    "        return 0\n",
    "    \n",
    "    #calculate f1 score\n",
    "    f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.616808Z",
     "start_time": "2019-02-12T16:04:22.608538Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(df):\n",
    "    \n",
    "    #mean of all results\n",
    "    accuracy = df[\"correct_result\"].mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.627593Z",
     "start_time": "2019-02-12T16:04:22.617991Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_precision(df):\n",
    "\n",
    "    #to count true positive\n",
    "    count_TP = 0\n",
    "    \n",
    "    #to count false positive\n",
    "    count_FP = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"result\"] == row[\"label\"] and row[\"label\"] == 1:\n",
    "            count_TP += 1\n",
    "        elif row[\"result\"] == 1 and row[\"label\"] == 0:\n",
    "            count_FP += 1\n",
    "    \n",
    "    #To check whether precision is defined or not. If not then return 0\n",
    "    if count_TP == 0 and count_FP == 0 :\n",
    "        return 0\n",
    "    \n",
    "    precision = (count_TP)/(count_TP + count_FP)\n",
    "    \n",
    "    return precision       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.637897Z",
     "start_time": "2019-02-12T16:04:22.628825Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_recall(df):\n",
    "    \n",
    "    #to count true positive\n",
    "    count_TP = 0\n",
    "    \n",
    "    #to count false negative\n",
    "    count_FN = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"result\"] == row[\"label\"] and row[\"label\"] == 1:\n",
    "            count_TP += 1\n",
    "        elif row[\"result\"] == 0 and row[\"label\"] == 1:    \n",
    "            count_FN += 1\n",
    "    \n",
    "    #To check whether precision is defined or not. If not then return 0\n",
    "    if count_TP == 0 and count_FN == 0 :\n",
    "        return 0\n",
    "    \n",
    "    recall = (count_TP)/(count_TP + count_FN)\n",
    "    \n",
    "    return recall        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on Train and Test Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.665117Z",
     "start_time": "2019-02-12T16:04:22.639201Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(filename):\n",
    "\n",
    "    train_df,validation_df = data_preprocessing()\n",
    "\n",
    "    data = train_df.values\n",
    "    label_column = train_df.values[:,-1]\n",
    "    feature_summary = train_classifier(train_df)\n",
    "\n",
    "    validation_df[\"result\"] = validation_df.apply(classify_data, args=(label_column,), axis=1)\n",
    "    validation_df[\"correct_result\"] = validation_df[\"result\"] == validation_df[\"label\"]\n",
    "\n",
    "    print()\n",
    "    print(\"Results on Validation Data\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Confusion Matrix\")\n",
    "    print_confusion_matrix(validation_df)\n",
    "    print(\"Accuracy: \",calculate_accuracy(validation_df))\n",
    "    print(\"Precision: \",calculate_precision(validation_df))\n",
    "    print(\"Recall: \",calculate_recall(validation_df))\n",
    "    print(\"F1 Score: \",calculate_f1_score(validation_df))\n",
    "    \n",
    "    \n",
    "    feature_list = [\"ID\",\"Age\",\"Experience\",\"Income\",\"Zipcode\",\"Family_Size\",\"Spending_Per_Month\",\"Education_Level\",\"Mortgage_Value\",\"Label\",\"Securities_Account\",\"Security_Deposits\",\"Internet_Banking\",\"Credit_Card\"]\n",
    "    \n",
    "    test_df = pd.read_csv(filename, names = feature_list)\n",
    "    test_df[\"temp\"] = test_df.Label\n",
    "    test_df = test_df.drop([\"Label\"], axis=1)\n",
    "    test_df[\"label\"] = test_df.temp\n",
    "    test_df = test_df.drop([\"temp\"], axis=1)\n",
    "    \n",
    "    test_df = test_df.drop([\"ID\",\"Zipcode\"], axis=1)\n",
    "    feature_list = [\"Age\",\"Experience\",\"Income\",\"Family_Size\",\"Spending_Per_Month\",\"Education_Level\",\"Mortgage_Value\",\"Securities_Account\",\"Security_Deposits\",\"Internet_Banking\",\"Credit_Card\",\"label\"]\n",
    "\n",
    "    \n",
    "    \n",
    "#     test_label_column = test_df.values[:,-1]\n",
    "    \n",
    "    test_df[\"result\"] = test_df.apply(classify_data, args=(label_column,), axis=1)\n",
    "    test_df[\"correct_result\"] = test_df[\"result\"] == test_df[\"label\"]\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print(\"Results on Test Data\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Confusion Matrix\")\n",
    "    print_confusion_matrix(test_df)\n",
    "    print(\"Accuracy: \",calculate_accuracy(test_df))\n",
    "    print(\"Precision: \",calculate_precision(test_df))\n",
    "    print(\"Recall: \",calculate_recall(test_df))\n",
    "    print(\"F1 Score: \",calculate_f1_score(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit Learn Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:22.705044Z",
     "start_time": "2019-02-12T16:04:22.666409Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_using_sklearn(filename):\n",
    "    \n",
    "    import time\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "    \n",
    "    feature_list = [\"ID\",\"Age\",\"Experience\",\"Income\",\"Zipcode\",\"Family_Size\",\"Spending_Per_Month\",\"Education_Level\",\"Mortgage_Value\",\"Label\",\"Securities_Account\",\"Security_Deposits\",\"Internet_Banking\",\"Credit_Card\"]\n",
    "\n",
    "    df = pd.read_csv(\"loan_dataset.csv\", names = feature_list)\n",
    "\n",
    "    df[\"temp\"] = df.Label\n",
    "    df = df.drop([\"Label\"], axis=1)\n",
    "    df[\"label\"] = df.temp\n",
    "    df = df.drop([\"temp\"], axis=1)\n",
    "\n",
    "    df = df.drop([\"ID\",\"Zipcode\"], axis=1)\n",
    "\n",
    "    feature_list = [\"Age\",\"Experience\",\"Income\",\"Family_Size\",\"Spending_Per_Month\",\"Education_Level\",\"Mortgage_Value\",\"Securities_Account\",\"Security_Deposits\",\"Internet_Banking\",\"Credit_Card\"]\n",
    "\n",
    "    X_train, X_test = train_test_split(df, test_size=0.3, random_state=int(time.time()))\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "\n",
    "    gnb.fit(\n",
    "        X_train[feature_list].values,\n",
    "        X_train[\"label\"]\n",
    "    )\n",
    "    y_pred = gnb.predict(X_test[feature_list])\n",
    "\n",
    "    print()\n",
    "    print(\"Results on Validation Data\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%\"\n",
    "          .format(\n",
    "              X_test.shape[0],\n",
    "              (X_test[\"label\"] != y_pred).sum(),\n",
    "              100*(1-(X_test[\"label\"] != y_pred).sum()/X_test.shape[0])\n",
    "    ))\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print(\"Results on Test Data\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    feature_list = [\"ID\",\"Age\",\"Experience\",\"Income\",\"Zipcode\",\"Family_Size\",\"Spending_Per_Month\",\"Education_Level\",\"Mortgage_Value\",\"Label\",\"Securities_Account\",\"Security_Deposits\",\"Internet_Banking\",\"Credit_Card\"]\n",
    "\n",
    "    test_df = pd.read_csv(filename, names = feature_list)\n",
    "\n",
    "    feature_list = [\"Age\",\"Experience\",\"Income\",\"Family_Size\",\"Spending_Per_Month\",\"Education_Level\",\"Mortgage_Value\",\"Securities_Account\",\"Security_Deposits\",\"Internet_Banking\",\"Credit_Card\"]\n",
    "\n",
    "    \n",
    "    test_df[\"temp\"] = test_df.Label\n",
    "    test_df = test_df.drop([\"Label\"], axis=1)\n",
    "    test_df[\"label\"] = test_df.temp\n",
    "    test_df = test_df.drop([\"temp\"], axis=1)\n",
    "\n",
    "    test_df = test_df.drop([\"ID\",\"Zipcode\"], axis=1)\n",
    "    \n",
    "    gnb.fit(\n",
    "        test_df[feature_list].values,\n",
    "        test_df[\"label\"]\n",
    "    )\n",
    "    \n",
    "    y_pred_test = gnb.predict(test_df[feature_list])\n",
    "    \n",
    "    print(\"Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%\"\n",
    "          .format(\n",
    "              test_df.shape[0],\n",
    "              (test_df[\"label\"] != y_pred_test).sum(),\n",
    "              100*(1-(test_df[\"label\"] != y_pred_test).sum()/test_df.shape[0])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Q 1 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T16:04:26.261204Z",
     "start_time": "2019-02-12T16:04:22.706360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on Validation Data\n",
      "\n",
      "Confusion Matrix\n",
      "True Positive:  58\n",
      "True Negative:  755\n",
      "False Positive:  58\n",
      "False Negative:  29\n",
      "Accuracy:  0.9033333333333333\n",
      "Precision:  0.5\n",
      "Recall:  0.6666666666666666\n",
      "F1 Score:  0.5714285714285715\n",
      "\n",
      "Results on Test Data\n",
      "\n",
      "Confusion Matrix\n",
      "True Positive:  273\n",
      "True Negative:  3809\n",
      "False Positive:  256\n",
      "False Negative:  161\n",
      "Accuracy:  0.9073127361635919\n",
      "Precision:  0.5160680529300568\n",
      "Recall:  0.6290322580645161\n",
      "F1 Score:  0.5669781931464174\n",
      "\n",
      "Result using Sklearn Library\n",
      "\n",
      "\n",
      "Results on Validation Data\n",
      "\n",
      "Number of mislabeled points out of a total 1350 points : 163, performance 87.93%\n",
      "\n",
      "Results on Test Data\n",
      "\n",
      "Number of mislabeled points out of a total 4499 points : 522, performance 88.40%\n"
     ]
    }
   ],
   "source": [
    "# test_file_name = sys.argv[1]\n",
    "\n",
    "test_file_name = \"loan_dataset.csv\"\n",
    "\n",
    "evaluate_model(test_file_name)\n",
    "\n",
    "print()\n",
    "print(\"Result using Sklearn Library\")\n",
    "print()\n",
    "\n",
    "train_and_evaluate_using_sklearn(test_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
