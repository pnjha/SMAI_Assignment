{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression One vs One and One vs All Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:39:49.320719Z",
     "start_time": "2019-02-17T12:39:49.313711Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ssl\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import heapq\n",
    "import itertools\n",
    "import random\n",
    "import copy\n",
    "from statistics import mean , stdev\n",
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Normalization of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:39:49.337937Z",
     "start_time": "2019-02-17T12:39:49.327271Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_scaling(train_data):\n",
    "    \n",
    "    no_of_columns = train_data.shape[1]\n",
    "    \n",
    "    sd_mean_list = []\n",
    "    \n",
    "    for index in range(no_of_columns):\n",
    "\n",
    "        sd_val = np.std(train_data[:,index])\n",
    "        mean_val = np.mean(train_data[:,index])\n",
    "        train_data[:,index] = (train_data[:,index] - mean_val)/(sd_val)\n",
    "        \n",
    "        sd_mean_list.append([sd_val,mean_val])\n",
    "        \n",
    "    return sd_mean_list, train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Normalization of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:39:49.349288Z",
     "start_time": "2019-02-17T12:39:49.339546Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_test_data(X_test,sd_mean_list):\n",
    "    \n",
    "    for test_row in X_test:\n",
    "        \n",
    "        for index in range(len(test_row)):\n",
    "\n",
    "            mean = sd_mean_list[index][1]\n",
    "            sd = sd_mean_list[index][0]\n",
    "\n",
    "            test_row[index] = (test_row[index] - mean)/sd\n",
    "\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:39:49.362855Z",
     "start_time": "2019-02-17T12:39:49.350813Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_preprocessing():\n",
    "    \n",
    "    feature_list = ['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','total sulfur dioxide','density','pH','sulphates','alcohol','quality']\n",
    "    \n",
    "    df = pd.read_csv(\"wine_datset.csv\",names = feature_list, dtype=np.float64, skiprows=1,sep=\";\")\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "\n",
    "    X = df.iloc[:,0:-1]\n",
    "    Y = df.iloc[:,-1]\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "    \n",
    "    Y_train = pd.DataFrame(Y_train)\n",
    "    Y_train = Y_train.values\n",
    "    \n",
    "    Y_test = pd.DataFrame(Y_test)\n",
    "    Y_test = Y_test.values\n",
    "    \n",
    "    sd_mean_list, X_train = feature_scaling(X_train.values)\n",
    "    \n",
    "    X_test = X_test.values\n",
    "\n",
    "\n",
    "    return sd_mean_list, X_train, X_test, Y_train, Y_test \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:39:49.373989Z",
     "start_time": "2019-02-17T12:39:49.364530Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(Y_predicted,Y_test):\n",
    "    \n",
    "    count = 0\n",
    "    total = 0\n",
    "    for i in range(len(Y_predicted)):\n",
    "        if Y_test[i] == Y_predicted[i]:\n",
    "            count += 1\n",
    "    \n",
    "    accuracy = count/len(Y_predicted)\n",
    "#     accuracy = (Y_predicted==Y_test).mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:39:49.414061Z",
     "start_time": "2019-02-17T12:39:49.375303Z"
    }
   },
   "outputs": [],
   "source": [
    "class Logistic_regression_model:\n",
    "\n",
    "    def __init__(self, learning_rate, iterations,threshold = 0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def calculate_sigmoid(self, exponent):\n",
    "        return 1/(1 + np.exp(-exponent))\n",
    "\n",
    "    def logistic_loss_function(self,y_predicted, y_actual):\n",
    "        return (-y_actual*np.log(y_predicted) - (1-y_actual) * np.log(1 - y_predicted)).mean()\n",
    "    \n",
    "\n",
    "    def concatenate_bias_column(self, data):\n",
    "        constants = np.ones((data.shape[0], 1))\n",
    "        return np.concatenate((constants, data), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, train_data, y_actual):\n",
    "        \n",
    "        self.loss_value = []\n",
    "        self.iteration = []\n",
    "\n",
    "        train_data = self.concatenate_bias_column(train_data)\n",
    "        \n",
    "        self.parameters = np.zeros(train_data.shape[1])\n",
    "        \n",
    "        \n",
    "        for index in range(self.iterations):\n",
    "            \n",
    "            y_inter = np.dot(train_data, self.parameters)\n",
    "        \n",
    "            y_predicted = self.calculate_sigmoid(y_inter)# >= self.threshold\n",
    "            \n",
    "            y_predicted = pd.DataFrame(y_predicted)\n",
    "            y_predicted = y_predicted.values\n",
    "\n",
    "            gradient = np.dot(train_data.T, (y_predicted - y_actual)) / y_actual.size\n",
    "            \n",
    "            self.parameters = pd.DataFrame(self.parameters)\n",
    "            self.parameters = self.parameters.values\n",
    "\n",
    "            self.parameters = self.parameters - self.learning_rate * gradient\n",
    "            \n",
    "\n",
    "            if(index%10 == 0 and index<150):\n",
    "                \n",
    "                y_inter = np.dot(train_data, self.parameters)\n",
    "                y_predicted = self.calculate_sigmoid(y_inter)\n",
    "                self.loss_value.append(self.logistic_loss_function(y_predicted, y_actual))\n",
    "                self.iteration.append(index)\n",
    "                # print(self.logistic_loss_function(y_predicted, y_actual))\n",
    "                \n",
    "    \n",
    "    def predict(self, data_row, method):\n",
    "\n",
    "        data = data_row.tolist()\n",
    "        data = [1] + data\n",
    "\n",
    "        data = np.asmatrix(data)\n",
    "\n",
    "        if method == \"ova\":\n",
    "            return self.calculate_sigmoid(np.dot(data, self.parameters))\n",
    "        elif method == \"ovo\":\n",
    "            return self.calculate_sigmoid(np.dot(data, self.parameters)) >= self.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression One vs All Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:39:49.438735Z",
     "start_time": "2019-02-17T12:39:49.416773Z"
    }
   },
   "outputs": [],
   "source": [
    "class logistic_regression_one_vs_all:\n",
    "    \n",
    "    def __init__(self, learning_rate, iterations,threshold):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def fit(self,X_train,Y_train):\n",
    "\n",
    "        unique_classes, counts = np.unique(Y_train,return_counts=True)\n",
    "\n",
    "        self.class_model_dict = {}\n",
    "\n",
    "        for a_class in unique_classes:\n",
    "\n",
    "            Y_train_modified = np.where(Y_train == a_class,1,0)\n",
    "        \n",
    "            self.class_model_dict[a_class] = Logistic_regression_model(self.learning_rate,self.iterations,self.threshold)\n",
    "            self.class_model_dict[a_class].fit(X_train, Y_train_modified)\n",
    "\n",
    "    def predict(self,X_test):\n",
    "        \n",
    "        y_predicted = []\n",
    "\n",
    "        for data_row in X_test:\n",
    "\n",
    "            max_probabilty = float('-inf')\n",
    "            class_label = -1\n",
    "\n",
    "            for a_class,model in self.class_model_dict.items():\n",
    "\n",
    "                label_probability = model.predict(data_row,\"ova\")\n",
    "\n",
    "                if label_probability > max_probabilty:\n",
    "\n",
    "                    max_probabilty = label_probability\n",
    "                    class_label = a_class\n",
    "\n",
    "            y_predicted.append(class_label)\n",
    "\n",
    "        y_predicted = np.asarray(y_predicted)\n",
    "    \n",
    "        return y_predicted\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Model One vs All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:39:49.455680Z",
     "start_time": "2019-02-17T12:39:49.440909Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_evaluate_logistic_one_vs_all():\n",
    "    \n",
    "    sd_mean_list, X_train, X_test, Y_train, Y_test = data_preprocessing()\n",
    "    X_test = scale_test_data(X_test,sd_mean_list)\n",
    "\n",
    "    lr_model = logistic_regression_one_vs_all(0.1,5000,0.7)\n",
    "\n",
    "    lr_model.fit(X_train,Y_train)\n",
    "\n",
    "    y_predicted = lr_model.predict(X_test)\n",
    "\n",
    "    confusion_matrix = metrics.cluster.contingency_matrix(Y_test, y_predicted)\n",
    "\n",
    "    print(\"One vs All Logistic Regression Model using Custom Implementation\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix)\n",
    "    print()\n",
    "    print(\"Accuracy: \",calculate_accuracy(y_predicted,Y_test))\n",
    "    print()\n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train,Y_train)\n",
    "    y_pre = clf.predict(X_test)\n",
    "\n",
    "    confusion_matrix = metrics.cluster.contingency_matrix(Y_test,y_pre)\n",
    "    print(\"One vs All Logistic Regression Model using Sklearn Multinomial Model\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix)\n",
    "    print()\n",
    "    print(\"Accuracy: \",calculate_accuracy(y_pre,Y_test))\n",
    "    print()\n",
    "#     clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression One vs One Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:39:49.492598Z",
     "start_time": "2019-02-17T12:39:49.457199Z"
    }
   },
   "outputs": [],
   "source": [
    "class logistic_regression_one_vs_one:\n",
    "    \n",
    "    def __init__(self,learning_rate,iterations,threshold):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.threshold = threshold\n",
    "        self.class_model_dict = {}\n",
    "\n",
    "    def fit(self,X_train,Y_train):\n",
    "\n",
    "        unique_classes, counts = np.unique(Y_train,return_counts=True)\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        for a_class in unique_classes:\n",
    "\n",
    "            j = i+1\n",
    "             \n",
    "            for b_class in unique_classes:\n",
    "\n",
    "                if j > i:\n",
    "                    \n",
    "                    indices = [i for i in range(len(Y_train)) if Y_train[i] != a_class and Y_train[i] != b_class]\n",
    "\n",
    "                    Y_train_modified = np.delete(Y_train, indices, 0)\n",
    "                    \n",
    "                    Y_train_modified = np.where(Y_train_modified == a_class,1,0)\n",
    "\n",
    "                    X_train_modified = np.delete(X_train,indices,0)\n",
    "                    \n",
    "                    self.class_model_dict[(a_class,b_class)] = Logistic_regression_model(self.learning_rate,self.iterations,self.threshold)\n",
    "                    \n",
    "                    self.class_model_dict[(a_class,b_class)].fit(X_train_modified, Y_train_modified)\n",
    "                    \n",
    "            \n",
    "                j += 1\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "    def predict(self,X_test):\n",
    "    \n",
    "        y_predicted = []\n",
    "        \n",
    "        for row in X_test:\n",
    "            \n",
    "            label_votes = []\n",
    "        \n",
    "            for label_tuple, model in self.class_model_dict.items():\n",
    "            \n",
    "                label_prediction = model.predict(row,\"ovo\")\n",
    "        \n",
    "                if label_prediction == 1:\n",
    "                \n",
    "                    label_votes.append(label_tuple[0])\n",
    "                else:\n",
    "                    label_votes.append(label_tuple[1])\n",
    "                    \n",
    "            y_predicted.append(max(set(label_votes), key=label_votes.count))\n",
    "            \n",
    "        y_predicted = np.asarray(y_predicted)\n",
    "    \n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:13:44.217478Z",
     "start_time": "2019-02-17T12:13:44.211683Z"
    }
   },
   "source": [
    "## Train and Evaluate Logistic Model One vs One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:39:49.515116Z",
     "start_time": "2019-02-17T12:39:49.494926Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_evaluate_logistic_one_vs_one():\n",
    "    \n",
    "    sd_mean_list, X_train, X_test, Y_train, Y_test = data_preprocessing()\n",
    "    X_test = scale_test_data(X_test,sd_mean_list)\n",
    "\n",
    "    lrovo_model = logistic_regression_one_vs_one(0.001,5000,0.7)\n",
    "\n",
    "    lrovo_model.fit(X_train,Y_train)\n",
    "\n",
    "    y_predicted = lrovo_model.predict(X_test)\n",
    "\n",
    "    confusion_matrix = metrics.cluster.contingency_matrix(Y_test, y_predicted)\n",
    "\n",
    "    print(\"One vs One Logistic Regression Model using Custom Implementation\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    print()\n",
    "    print(confusion_matrix)\n",
    "    print()\n",
    "    print(\"Accuracy: \",calculate_accuracy(y_predicted,Y_test))\n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train,Y_train)\n",
    "    y_pre = clf.predict(X_test)\n",
    "\n",
    "    confusion_matrix = metrics.cluster.contingency_matrix(Y_test,y_pre)\n",
    "    print(\"One vs One Logistic Regression Model using Sklearn Multinomial Model\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix)\n",
    "    print()\n",
    "    print(\"Accuracy: \",calculate_accuracy(y_pre,Y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T12:41:28.482332Z",
     "start_time": "2019-02-17T12:39:49.516578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs All Logistic Regression Model using Custom Implementation\n",
      "Confusion Matrix\n",
      "[[  0   2   2   0]\n",
      " [  0  18  13   0]\n",
      " [  0 132 138   0]\n",
      " [  1  60 298  11]\n",
      " [  0   3 143  22]\n",
      " [  0   0  34   4]\n",
      " [  0   0   0   1]]\n",
      "\n",
      "Accuracy:  0.5124716553287982\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakashjha/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/prakashjha/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs All Logistic Regression Model using Sklearn Multinomial Model\n",
      "Confusion Matrix\n",
      "[[  0   2   2   0]\n",
      " [  2  16  13   0]\n",
      " [  0 134 135   1]\n",
      " [  1  59 280  30]\n",
      " [  0   6 131  31]\n",
      " [  0   0  28  10]\n",
      " [  0   0   0   1]]\n",
      "\n",
      "Accuracy:  0.5068027210884354\n",
      "\n",
      "One vs One Logistic Regression Model using Custom Implementation\n",
      "Confusion Matrix\n",
      "\n",
      "[[  0   2   2   0]\n",
      " [  2  21   7   1]\n",
      " [  5 212  52   1]\n",
      " [  3 148 212   7]\n",
      " [  0  30 130   8]\n",
      " [  0   6  25   7]\n",
      " [  0   0   1   0]]\n",
      "\n",
      "Accuracy:  0.49206349206349204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakashjha/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs One Logistic Regression Model using Sklearn Multinomial Model\n",
      "Confusion Matrix\n",
      "[[  0   2   2   0]\n",
      " [  2  16  13   0]\n",
      " [  0 134 135   1]\n",
      " [  1  59 280  30]\n",
      " [  0   6 131  31]\n",
      " [  0   0  28  10]\n",
      " [  0   0   0   1]]\n",
      "\n",
      "Accuracy:  0.5068027210884354\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakashjha/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "train_evaluate_logistic_one_vs_all()\n",
    "train_evaluate_logistic_one_vs_one()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
